{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e1a5a6b-cee8-453d-bf50-76e22271beb4",
   "metadata": {},
   "source": [
    "Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5e63ee5-2888-474a-9547-b2642fd3a3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation_date    0\n",
      "CPIAUCSL            0\n",
      "dtype: int64\n",
      "observation_date    0\n",
      "FEDFUNDS            0\n",
      "dtype: int64\n",
      "observation_date    0\n",
      "GDP                 0\n",
      "dtype: int64\n",
      "observation_date    0\n",
      "UMCSENT             0\n",
      "dtype: int64\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('cleaned_lending_club_data.csv')\n",
    "\n",
    "##### STEP 10 ##### Loading the other datasets\n",
    "\n",
    "# Load economic indicators\n",
    "cpi = pd.read_csv('CPIAUCSL.csv')\n",
    "fedfunds = pd.read_csv('FEDFUNDS.csv')\n",
    "gdp = pd.read_csv('GDP.csv')\n",
    "sentiment = pd.read_csv('UMCSENT.csv')\n",
    "\n",
    "print(cpi.isnull().sum())\n",
    "print(fedfunds.isnull().sum())\n",
    "print(gdp.isnull().sum())\n",
    "print(sentiment.isnull().sum())\n",
    "\n",
    "# Load housing data \n",
    "zhvi_zip_raw = pd.read_csv('zvhi_zip.csv')\n",
    "zhvi_state_raw = pd.read_csv('zvhi_state.csv')\n",
    "\n",
    "print(gdp.isnull().sum().sum())\n",
    "print(sentiment.isnull().sum().sum())\n",
    "\n",
    "df['issue_d'] = pd.to_datetime(df['issue_d'], format='%b-%Y')\n",
    "df['year_month'] = df['issue_d'].dt.to_period('M').astype(str)\n",
    "df['zip_3digit'] = df['zip_code'].str[:3].astype(int) * 100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb277270-0933-4a25-8ea7-4e5b0ef2c62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Economic Indicators...\n",
      "Processing Housing Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagro\\AppData\\Local\\Temp\\ipykernel_75428\\3288331708.py:109: FutureWarning: The default fill_method='ffill' in SeriesGroupBy.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  zhvi_zip[f'ZHVI_pct_change_{k}'] = g.pct_change(k) * 100\n",
      "C:\\Users\\sagro\\AppData\\Local\\Temp\\ipykernel_75428\\3288331708.py:109: FutureWarning: The default fill_method='ffill' in SeriesGroupBy.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  zhvi_zip[f'ZHVI_pct_change_{k}'] = g.pct_change(k) * 100\n",
      "C:\\Users\\sagro\\AppData\\Local\\Temp\\ipykernel_75428\\3288331708.py:109: FutureWarning: The default fill_method='ffill' in SeriesGroupBy.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  zhvi_zip[f'ZHVI_pct_change_{k}'] = g.pct_change(k) * 100\n",
      "C:\\Users\\sagro\\AppData\\Local\\Temp\\ipykernel_75428\\3288331708.py:109: FutureWarning: The default fill_method='ffill' in SeriesGroupBy.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  zhvi_zip[f'ZHVI_pct_change_{k}'] = g.pct_change(k) * 100\n",
      "C:\\Users\\sagro\\AppData\\Local\\Temp\\ipykernel_75428\\3288331708.py:117: FutureWarning: The default fill_method='ffill' in SeriesGroupBy.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  zhvi_zip['ZHVI_return'] = g.pct_change()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging data...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 11: Data Preparation and Pre-Merge Feature Engineering\n",
    "# ============================================================================\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1. Prepare Main Dataframe Key Columns\n",
    "# ---------------------------------------------------------------------------\n",
    "df['issue_d'] = pd.to_datetime(df['issue_d'], format='%b-%Y')\n",
    "df['year_month'] = df['issue_d'].dt.to_period('M') # Keep as Period object for easy merging\n",
    "df['zip_3digit'] = df['zip_code'].str[:3].astype(int) * 100\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2. Process & Engineer Economic Indicators (PRE-MERGE)\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\"Processing Economic Indicators...\")\n",
    "\n",
    "# Standardize Dates\n",
    "for econ_df, name in [(cpi, 'CPI'), (fedfunds, 'FEDFUNDS'), (gdp, 'GDP'), (sentiment, 'UMCSENT')]:\n",
    "    econ_df['observation_date'] = pd.to_datetime(econ_df['observation_date'])\n",
    "    econ_df['year_month'] = econ_df['observation_date'].dt.to_period('M')\n",
    "    econ_df.rename(columns={econ_df.columns[1]: name}, inplace=True)\n",
    "\n",
    "# Resample GDP to Monthly (Forward Fill)\n",
    "gdp = gdp.set_index('observation_date').resample('MS').ffill().reset_index()\n",
    "gdp['year_month'] = gdp['observation_date'].dt.to_period('M')\n",
    "\n",
    "# Merge all econ data into one lightweight dataframe\n",
    "econ_data = cpi[['year_month', 'CPI']]\n",
    "econ_data = econ_data.merge(fedfunds[['year_month', 'FEDFUNDS']], on='year_month', how='outer')\n",
    "econ_data = econ_data.merge(gdp[['year_month', 'GDP']], on='year_month', how='outer')\n",
    "econ_data = econ_data.merge(sentiment[['year_month', 'UMCSENT']], on='year_month', how='outer')\n",
    "\n",
    "# Sort by time is strictly required for shifting\n",
    "econ_data = econ_data.sort_values('year_month').reset_index(drop=True)\n",
    "econ_data = econ_data.ffill().bfill() # Handle any small gaps from outer joins\n",
    "\n",
    "# --- ENGINEER MACRO FEATURES ---\n",
    "indicators = ['CPI', 'FEDFUNDS', 'GDP', 'UMCSENT']\n",
    "\n",
    "for col in indicators:\n",
    "    # Lags\n",
    "    for k in [1, 3, 6, 12]:\n",
    "        econ_data[f'{col}_lag_{k}'] = econ_data[col].shift(k)\n",
    "    \n",
    "    # Moving Averages\n",
    "    for n in [3, 6, 12]:\n",
    "        econ_data[f'{col}_ma_{n}'] = econ_data[col].rolling(window=n, min_periods=1).mean()\n",
    "    \n",
    "    # Rate of Change (Percentage)\n",
    "    for k in [1, 3, 6, 12]:\n",
    "        econ_data[f'{col}_pct_change_{k}'] = econ_data[col].pct_change(k) * 100\n",
    "\n",
    "# Composite Econ Indicators\n",
    "# 1. Real Interest Rate\n",
    "econ_data['CPI_inflation_12m'] = econ_data['CPI'].pct_change(12) * 100\n",
    "econ_data['real_interest_rate'] = econ_data['FEDFUNDS'] - econ_data['CPI_inflation_12m']\n",
    "\n",
    "# 2. Fed Stance\n",
    "econ_data['fed_stance'] = econ_data['FEDFUNDS'] - econ_data['FEDFUNDS_ma_12']\n",
    "\n",
    "# 3. Economic Momentum\n",
    "econ_data['economic_momentum'] = (econ_data['CPI_pct_change_3'] + \n",
    "                                  econ_data['GDP_pct_change_3'] + \n",
    "                                  econ_data['UMCSENT_pct_change_3']) / 3\n",
    "\n",
    "# 4. Economic Stress (Fed rising while Sentiment/GDP falling)\n",
    "# Note: We need housing change for the full equation, but we calculate partial here\n",
    "econ_data['economic_stress_partial'] = (econ_data['FEDFUNDS_pct_change_3'] - \n",
    "                                        econ_data['UMCSENT_pct_change_3']) / 2\n",
    "\n",
    "# Fill NaNs generated by shifting (First few rows will be NaN)\n",
    "econ_data = econ_data.bfill()\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3. Process & Engineer Housing Data (PRE-MERGE)\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\"Processing Housing Data...\")\n",
    "\n",
    "# -- Helper function to clean ZHVI data --\n",
    "def process_zhvi(raw_df, region_col, value_col_name):\n",
    "    # Identify date columns (YYYY-MM-DD)\n",
    "    date_cols = [col for col in raw_df.columns if col[0].isdigit()]\n",
    "    id_vars = [col for col in raw_df.columns if col not in date_cols]\n",
    "    \n",
    "    # Melt to long format\n",
    "    df_long = raw_df.melt(id_vars=id_vars, value_vars=date_cols, \n",
    "                          var_name='date', value_name=value_col_name)\n",
    "    \n",
    "    df_long['date'] = pd.to_datetime(df_long['date'])\n",
    "    df_long['year_month'] = df_long['date'].dt.to_period('M')\n",
    "    \n",
    "    return df_long\n",
    "\n",
    "# A. Process ZIP Data\n",
    "zhvi_zip_long = process_zhvi(zhvi_zip_raw, 'RegionName', 'ZHVI')\n",
    "zhvi_zip_long['zip_3digit'] = (zhvi_zip_long['RegionName'].astype(int) // 100) * 100\n",
    "\n",
    "# Aggregate to Zip-3 level\n",
    "zhvi_zip = zhvi_zip_long.groupby(['zip_3digit', 'year_month'])['ZHVI'].mean().reset_index()\n",
    "\n",
    "# Sort for time series operations\n",
    "zhvi_zip = zhvi_zip.sort_values(['zip_3digit', 'year_month'])\n",
    "\n",
    "# --- ENGINEER HOUSING FEATURES (ZIP) ---\n",
    "# Group by Zip to ensure shifts don't bleed across regions\n",
    "g = zhvi_zip.groupby('zip_3digit')['ZHVI']\n",
    "\n",
    "for k in [1, 3, 6, 12]:\n",
    "    zhvi_zip[f'ZHVI_pct_change_{k}'] = g.pct_change(k) * 100\n",
    "    zhvi_zip[f'ZHVI_momentum_{k}'] = zhvi_zip[f'ZHVI_pct_change_{k}'] # Alias\n",
    "    zhvi_zip[f'ZHVI_lag_{k}'] = g.shift(k)\n",
    "\n",
    "for n in [3, 6, 12]:\n",
    "    zhvi_zip[f'ZHVI_ma_{n}'] = g.rolling(window=n, min_periods=1).mean().reset_index(0, drop=True)\n",
    "\n",
    "# Volatility & Trend\n",
    "zhvi_zip['ZHVI_return'] = g.pct_change()\n",
    "zhvi_zip['housing_volatility'] = zhvi_zip.groupby('zip_3digit')['ZHVI_return'].transform(\n",
    "    lambda x: x.rolling(window=6, min_periods=1).std()\n",
    ")\n",
    "zhvi_zip['housing_trend_signal'] = (zhvi_zip['ZHVI'] > zhvi_zip['ZHVI_ma_12']).astype(int)\n",
    "\n",
    "# B. Process State Data (Fallback)\n",
    "zhvi_state_long = process_zhvi(zhvi_state_raw, 'RegionName', 'ZHVI_state')\n",
    "zhvi_state = zhvi_state_long.rename(columns={'RegionName': 'state'})[['state', 'year_month', 'ZHVI_state']]\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4. JOINS: Merge Enriched Data into Main DataFrame\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\"Merging data...\")\n",
    "\n",
    "# Join 1: Economic Data\n",
    "df = df.merge(econ_data, on='year_month', how='left')\n",
    "\n",
    "# Join 2: ZIP Housing Data\n",
    "df = df.merge(zhvi_zip, on=['zip_3digit', 'year_month'], how='left')\n",
    "\n",
    "# Join 3: State Housing Data (Fallback)\n",
    "df = df.merge(zhvi_state, left_on=['addr_state', 'year_month'], \n",
    "              right_on=['state', 'year_month'], how='left')\n",
    "\n",
    "# Apply Fallback: If Zip ZHVI is missing, use State ZHVI\n",
    "df['ZHVI'] = df['ZHVI'].fillna(df['ZHVI_state'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63411bb2-ca33-4729-b81c-0c856780b04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering Interaction Features...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 12: Interaction Feature Engineering (Post-Merge)\n",
    "# ============================================================================\n",
    "print(\"Engineering Interaction Features...\")\n",
    "\n",
    "# 1. Finish Economic Stress (requires merged Housing data)\n",
    "# Note: We check if column exists to handle potential NaNs from merge\n",
    "df['economic_stress'] = (df['economic_stress_partial'] * 2 - df['ZHVI_pct_change_3']) / 3\n",
    "\n",
    "# 2. Interest Rate Spread\n",
    "df['interest_rate_spread'] = df['int_rate'] - df['FEDFUNDS']\n",
    "\n",
    "# 3. Debt-to-Income Adjusted\n",
    "# Assume monthly housing payment proxy is 0.4% of home value\n",
    "df['estimated_housing_payment'] = 0.004 * df['ZHVI']\n",
    "df['dti_adjusted'] = df['dti'] + (12 * df['estimated_housing_payment'] / df['annual_inc']) * 100\n",
    "\n",
    "# 4. LTV Proxy (Loan Amount / Home Value)\n",
    "df['ltv_proxy'] = df['loan_amnt'] / df['ZHVI']\n",
    "\n",
    "# 5. Composite Risk Score\n",
    "df['composite_risk_score'] = (df['dti']/100 + \n",
    "                              df['delinq_2yrs']/10 + \n",
    "                              df['interest_rate_spread']/20) / 3\n",
    "\n",
    "# 6. Credit History Length\n",
    "df['earliest_cr_line'] = pd.to_datetime(df['earliest_cr_line'], format='%b-%Y')\n",
    "df['credit_history_length_months'] = ((df['issue_d'] - df['earliest_cr_line']).dt.days / 30.44).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "227f6e5a-1dd4-4211-a459-a141af58e833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 13: Handling Missing Values...\n",
      "Imputing 24 columns with median values...\n",
      "  - ZHVI: filled 1688 rows with 215634.8132\n",
      "  - ZHVI_pct_change_1: filled 1726 rows with 0.4676\n",
      "  - ZHVI_momentum_1: filled 1726 rows with 0.4676\n",
      "  - ZHVI_lag_1: filled 1727 rows with 214510.8077\n",
      "  - ZHVI_pct_change_3: filled 1838 rows with 1.3953\n",
      "  - ZHVI_momentum_3: filled 1838 rows with 1.3953\n",
      "  - ZHVI_lag_3: filled 1840 rows with 212244.5878\n",
      "  - ZHVI_pct_change_6: filled 1984 rows with 2.7955\n",
      "  - ZHVI_momentum_6: filled 1984 rows with 2.7955\n",
      "  - ZHVI_lag_6: filled 1992 rows with 209190.7112\n",
      "  - ZHVI_pct_change_12: filled 2319 rows with 5.4781\n",
      "  - ZHVI_momentum_12: filled 2319 rows with 5.4781\n",
      "  - ZHVI_lag_12: filled 2322 rows with 203486.6362\n",
      "  - ZHVI_ma_3: filled 1688 rows with 214475.1541\n",
      "  - ZHVI_ma_6: filled 1687 rows with 212743.9710\n",
      "  - ZHVI_ma_12: filled 1687 rows with 209758.1275\n",
      "  - ZHVI_return: filled 1726 rows with 0.0047\n",
      "  - housing_volatility: filled 1790 rows with 0.0017\n",
      "  - housing_trend_signal: filled 604 rows with 1.0000\n",
      "  - ZHVI_state: filled 1306387 rows with nan\n",
      "  - economic_stress: filled 1838 rows with 3.1951\n",
      "  - estimated_housing_payment: filled 1688 rows with 862.5393\n",
      "  - dti_adjusted: filled 1688 rows with 36.6341\n",
      "  - ltv_proxy: filled 1688 rows with 0.0528\n",
      "\n",
      "=== Post-Imputation Check ===\n",
      "Final dataset shape: (1306387, 161)\n",
      "Total missing values: 2612774\n",
      "WARNING: Remaining missing values found:\n",
      "state         1306387\n",
      "ZHVI_state    1306387\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 13: Handle Missing Values\n",
    "# ============================================================================\n",
    "print(\"Step 13: Handling Missing Values...\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1. Create Missing Indicators\n",
    "# ---------------------------------------------------------------------------\n",
    "# We flag these because a missing ZHVI often indicates a rural area\n",
    "# or a non-standard housing market, which is predictive of risk.\n",
    "zhvi_cols = ['ZHVI', 'ZHVI_lag_1', 'ZHVI_lag_3', 'ZHVI_lag_6', 'ZHVI_lag_12']\n",
    "for col in zhvi_cols:\n",
    "    if col in df.columns:\n",
    "        # Create boolean flag (1 if missing, 0 otherwise)\n",
    "        df[f'{col}_missing'] = df[col].isnull().astype(int)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2. Median Imputation\n",
    "# ---------------------------------------------------------------------------\n",
    "# Rationale: We already used State-level fallbacks in Step 11. \n",
    "# The remaining gaps (<0.1% of data) are edge cases. \n",
    "# Complex time-series imputation here is computationally expensive and \n",
    "# offers diminishing returns over median imputation.\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "cols_with_missing = [col for col in numeric_cols if df[col].isnull().any()]\n",
    "\n",
    "if cols_with_missing:\n",
    "    print(f\"Imputing {len(cols_with_missing)} columns with median values...\")\n",
    "    \n",
    "    # Calculate medians once (efficent)\n",
    "    fill_values = df[cols_with_missing].median()\n",
    "    \n",
    "    # Log what we are filling for sanity check\n",
    "    for col, val in fill_values.items():\n",
    "        missing_count = df[col].isnull().sum()\n",
    "        if missing_count > 0:\n",
    "            print(f\"  - {col}: filled {missing_count} rows with {val:.4f}\")\n",
    "            \n",
    "    # Apply fill\n",
    "    df[cols_with_missing] = df[cols_with_missing].fillna(fill_values)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3. Final Verification\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\"\\n=== Post-Imputation Check ===\")\n",
    "print(f\"Final dataset shape: {df.shape}\")\n",
    "print(f\"Total missing values: {df.isnull().sum().sum()}\")\n",
    "\n",
    "# Sanity Check\n",
    "if df.isnull().sum().sum() > 0:\n",
    "    print(\"WARNING: Remaining missing values found:\")\n",
    "    print(df.isnull().sum()[df.isnull().sum() > 0])\n",
    "else:\n",
    "    print(\"SUCCESS: No missing values remaining.\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4. Save Final Dataset (Optional but recommended)\n",
    "# ---------------------------------------------------------------------------\n",
    "# df.to_csv('data/lc_data_with_economic_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f57337e2-cd18-4a98-a444-f68ee75be72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining empty columns detected: ['state', 'ZHVI_state']\n",
      "2 columns dropped.\n",
      "Total missing values FINAL: 0\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SAFETY CHECK: Dropping remaining columns with missing values\n",
    "# ============================================================================\n",
    "\n",
    "# 1. Identify columns that still have missing values\n",
    "cols_to_drop = df.columns[df.isnull().any()].tolist()\n",
    "print(f\"Remaining empty columns detected: {cols_to_drop}\")\n",
    "\n",
    "# 2. Drop them\n",
    "if cols_to_drop:\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "    print(f\"{len(cols_to_drop)} columns dropped.\")\n",
    "else:\n",
    "    print(\"No remaining empty columns.\")\n",
    "\n",
    "# 3. Final check (Must print 0)\n",
    "print(f\"Total missing values FINAL: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8232d9cf-d046-47aa-963e-1805fc87fffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 4 duplicate 'pct_change' columns (kept 'momentum').\n",
      "Removed 4 structural helper columns.\n",
      "\n",
      "============================================================\n",
      "FINAL DATASET READY FOR SPLITTING\n",
      "============================================================\n",
      "Rows: 1,306,387\n",
      "Features: 151\n",
      "Missing values: 0\n",
      "Date Range: 2007-06-01 to 2018-12-01\n",
      "(NOTE: 'issue_d' preserved for time-based train/test splitting)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 14: Final Cleanup and Export\n",
    "# ============================================================================\n",
    "\n",
    "# 1. Remove Duplicate Features\n",
    "# ----------------------------------------------------------------------------\n",
    "# We created 'ZHVI_momentum' as an alias for 'ZHVI_pct_change' in Step 11.\n",
    "# We will keep 'momentum' as it sounds more descriptive for a risk feature.\n",
    "duplicate_cols = [f'ZHVI_pct_change_{k}' for k in [1, 3, 6, 12]]\n",
    "cols_to_drop_dup = [col for col in duplicate_cols if col in df.columns]\n",
    "\n",
    "if cols_to_drop_dup:\n",
    "    df = df.drop(columns=cols_to_drop_dup)\n",
    "    print(f\"Removed {len(cols_to_drop_dup)} duplicate 'pct_change' columns (kept 'momentum').\")\n",
    "\n",
    "# 2. Remove Structural/Temporal Helper Columns\n",
    "# ----------------------------------------------------------------------------\n",
    "# We drop columns used for joining (zip_3digit, year_month) \n",
    "# and raw location identifiers (zip_code, addr_state).\n",
    "#\n",
    "# NOTE: We keep 'issue_d' for the Train/Test split (Time-based splitting).\n",
    "structural_cols = [\n",
    "    'year_month', \n",
    "    'zip_3digit', \n",
    "    'zip_code', \n",
    "    'addr_state', \n",
    "    'state',\n",
    "    'ZHVI_state'\n",
    "    'earliest_cr_line' # We already engineered 'credit_history_length_months'\n",
    "]\n",
    "\n",
    "cols_to_drop_struct = [col for col in structural_cols if col in df.columns]\n",
    "if cols_to_drop_struct:\n",
    "    df = df.drop(columns=cols_to_drop_struct)\n",
    "    print(f\"Removed {len(cols_to_drop_struct)} structural helper columns.\")\n",
    "\n",
    "# 3. Final Data Check\n",
    "# ----------------------------------------------------------------------------\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FINAL DATASET READY FOR SPLITTING\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Rows: {df.shape[0]:,}\")\n",
    "print(f\"Features: {df.shape[1]}\")\n",
    "print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
    "\n",
    "# Check if 'issue_d' is present (Required for valid backtesting)\n",
    "if 'issue_d' in df.columns:\n",
    "    print(f\"Date Range: {df['issue_d'].min().date()} to {df['issue_d'].max().date()}\")\n",
    "    print(\"(NOTE: 'issue_d' preserved for time-based train/test splitting)\")\n",
    "else:\n",
    "    print(\"WARNING: 'issue_d' was dropped. Ensure you have already split your data!\")\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# 4. Save to Parquet (Better than CSV for large data types)\n",
    "# ----------------------------------------------------------------------------\n",
    "# df.to_parquet('data/lc_final_features.parquet')\n",
    "# print(\"Dataset saved to 'data/lc_final_features.parquet'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e55520f-bd6e-4c6c-b268-7dc85cbf931a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f6f7d45-3f5d-4d77-819d-310bf4cb537a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dataset sauvegardé avec succès via fastparquet\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 4. Save Final Dataset\n",
    "# ============================================================================\n",
    "\n",
    "df.to_parquet(\n",
    "    r'C:\\ESILV A4\\ESILV A4 DIA\\Machine Learning\\Projet ML\\final_dataset_enhanced.parquet',\n",
    "    index=False,\n",
    "    engine='fastparquet' \n",
    ")\n",
    "print(\" Dataset sauvegardé avec succès via fastparquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ca1da69-9835-4e54-88a2-f7754c7d163b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Generating Correlation Heatmap ===\n",
      "Calculating correlations for 148 features...\n",
      "Found 82 features with |correlation| > 0.7\n",
      "✓ Saved heatmap to: ML/correlation_heatmap.png\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 15: Correlation Heatmap\n",
    "# ============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n=== Generating Correlation Heatmap ===\")\n",
    "\n",
    "# Get numerical columns (exclude target)\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if 'loan_status_binary' in numerical_cols:\n",
    "    numerical_cols.remove('loan_status_binary')\n",
    "\n",
    "print(f\"Calculating correlations for {len(numerical_cols)} features...\")\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = df[numerical_cols].corr()\n",
    "\n",
    "# Filter to features with at least one high correlation (to reduce clutter)\n",
    "THRESHOLD = 0.7\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "high_corr_features = set()\n",
    "\n",
    "for col in upper_tri.columns:\n",
    "    if any(abs(upper_tri[col]) > THRESHOLD):\n",
    "        high_corr_features.add(col)\n",
    "\n",
    "high_corr_features = sorted(list(high_corr_features))\n",
    "print(f\"Found {len(high_corr_features)} features with |correlation| > {THRESHOLD}\")\n",
    "\n",
    "# Create heatmap\n",
    "if len(high_corr_features) > 0:\n",
    "    filtered_corr = corr_matrix.loc[high_corr_features, high_corr_features]\n",
    "\n",
    "    plt.figure(figsize=(max(16, len(high_corr_features)*0.5), max(14, len(high_corr_features)*0.4)))\n",
    "    sns.heatmap(\n",
    "        filtered_corr,\n",
    "        cmap='coolwarm',\n",
    "        center=0,\n",
    "        vmin=-1,\n",
    "        vmax=1,\n",
    "        annot=False,\n",
    "        linewidths=0.5,\n",
    "        square=True,\n",
    "        cbar_kws={'label': 'Correlation Coefficient'}\n",
    "    )\n",
    "    plt.title(f'Feature Correlation Heatmap (|r| > {THRESHOLD})', fontsize=16, fontweight='bold')\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "    plt.yticks(rotation=0, fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('C:\\\\ESILV A4\\\\ESILV A4 DIA\\\\Machine Learning\\\\Projet ML\\\\correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"✓ Saved heatmap to: ML/correlation_heatmap.png\")\n",
    "    plt.close()\n",
    "else:\n",
    "    print(\"No features with high correlation found\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9fd9547-1db6-4df8-a28f-c3ac55619e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Data Validation ===\n",
      "\n",
      "⚠️  Found inf values in 1 columns:\n",
      "  dti_adjusted: 302 inf values\n",
      "\n",
      "Replacing inf values with median...\n",
      "  dti_adjusted: replaced with median 36.63\n",
      "✓ Data validation passed\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 16: Data Validation (Check for inf/extreme values)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n=== Data Validation ===\")\n",
    "\n",
    "# Check for inf values\n",
    "inf_counts = np.isinf(df.select_dtypes(include=[np.number])).sum()\n",
    "inf_cols = inf_counts[inf_counts > 0]\n",
    "\n",
    "if len(inf_cols) > 0:\n",
    "    print(f\"\\n⚠️  Found inf values in {len(inf_cols)} columns:\")\n",
    "    for col, count in inf_cols.items():\n",
    "        print(f\"  {col}: {count} inf values\")\n",
    "\n",
    "    # Replace inf with NaN, then use median imputation\n",
    "    print(\"\\nReplacing inf values with median...\")\n",
    "    for col in inf_cols.index:\n",
    "        # Replace inf with NaN\n",
    "        df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n",
    "        # Fill with median\n",
    "        median_val = df[col].median()\n",
    "        df[col] = df[col].fillna(median_val)\n",
    "        print(f\"  {col}: replaced with median {median_val:.2f}\")\n",
    "else:\n",
    "    print(\"✓ No inf values found\")\n",
    "\n",
    "# Verify no more inf/nan values\n",
    "assert not df.select_dtypes(include=[np.number]).isin([np.inf, -np.inf]).any().any(), \"Still have inf values!\"\n",
    "assert not df.isnull().any().any(), \"Still have NaN values!\"\n",
    "print(\"✓ Data validation passed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a19cbf-3b14-4d2d-b191-00638c4a8562",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
